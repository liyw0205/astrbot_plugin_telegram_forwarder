"""
æ¶ˆæ¯è½¬å‘æ ¸å¿ƒæ¨¡å—

è´Ÿè´£ä» Telegram é¢‘é“è·å–æ¶ˆæ¯å¹¶è½¬å‘åˆ°ç›®æ ‡å¹³å°ï¼ˆTelegram é¢‘é“æˆ– QQ ç¾¤ï¼‰ã€‚
ä¸»è¦åŠŸèƒ½ï¼š
- å®šæœŸæ£€æŸ¥é¢‘é“æ›´æ–°
- æ¶ˆæ¯è¿‡æ»¤ï¼ˆå…³é”®è¯ã€æ­£åˆ™è¡¨è¾¾å¼ï¼‰
- å†·å¯åŠ¨æ”¯æŒï¼ˆä»æŒ‡å®šæ—¥æœŸå¼€å§‹ï¼‰
- åª’ä½“æ–‡ä»¶ä¸‹è½½å’Œå¤„ç†
- å¤šå¹³å°æ¶ˆæ¯å‘é€
"""

import asyncio
import re
import os
import httpx
from datetime import datetime, timezone
from typing import Optional
from telethon.tl.types import Message, PeerUser

from astrbot.api import logger, AstrBotConfig
from ..common.text_tools import clean_telegram_text
from ..common.storage import Storage
from .client import TelegramClientWrapper

MAX_FILE_SIZE = 50 * 1024 * 1024  # 50MB

class Forwarder:
    """
    æ¶ˆæ¯è½¬å‘å™¨æ ¸å¿ƒç±»

    è´Ÿè´£ä» Telegram æºé¢‘é“è·å–æ¶ˆæ¯ï¼Œå¤„ç†åè½¬å‘åˆ°ç›®æ ‡å¹³å°ã€‚
    æ”¯æŒ Telegram-to-Telegram å’Œ Telegram-to-QQ ä¸¤ç§è½¬å‘æ¨¡å¼ã€‚
    """
    def __init__(self, config: AstrBotConfig, storage: Storage, client_wrapper: TelegramClientWrapper, plugin_data_dir: str):
        """
        åˆå§‹åŒ–è½¬å‘å™¨

        Args:
            config: AstrBot é…ç½®å¯¹è±¡ï¼ŒåŒ…å«æºé¢‘é“ã€ç›®æ ‡é¢‘é“ã€è¿‡æ»¤è§„åˆ™ç­‰
            storage: æ•°æ®æŒä¹…åŒ–ç®¡ç†å™¨ï¼Œç”¨äºè®°å½•å·²å¤„ç†çš„æ¶ˆæ¯ID
            client_wrapper: Telegram å®¢æˆ·ç«¯å°è£…
            plugin_data_dir: æ’ä»¶æ•°æ®ç›®å½•ï¼Œç”¨äºä¸´æ—¶å­˜å‚¨ä¸‹è½½çš„åª’ä½“æ–‡ä»¶
        """
        self.config = config
        self.storage = storage
        self.client_wrapper = client_wrapper
        self.client = client_wrapper.client  # å¿«æ·è®¿é—®
        self.plugin_data_dir = plugin_data_dir

    async def check_updates(self):
        """
        æ£€æŸ¥æ‰€æœ‰é…ç½®çš„é¢‘é“æ›´æ–°

        æ‰§è¡Œæµç¨‹ï¼š
        1. æ£€æŸ¥å®¢æˆ·ç«¯è¿æ¥çŠ¶æ€
        2. éå†æ‰€æœ‰é…ç½®çš„æºé¢‘é“
        3. è§£æé¢‘é“é…ç½®ï¼ˆæ”¯æŒæ—¥æœŸè¿‡æ»¤ï¼‰
        4. è°ƒç”¨ _process_channel å¤„ç†æ¯ä¸ªé¢‘é“

        é¢‘é“é…ç½®æ ¼å¼ï¼š
            - "channel_name" - ä»æœ€æ–°æ¶ˆæ¯å¼€å§‹
            - "channel_name|2024-01-01" - ä»æŒ‡å®šæ—¥æœŸå¼€å§‹

        å¼‚å¸¸å¤„ç†ï¼š
            - å•ä¸ªé¢‘é“å¤„ç†å¤±è´¥ä¸å½±å“å…¶ä»–é¢‘é“
            - æ¯ä¸ªé¢‘é“çš„é”™è¯¯ä¼šè¢«è®°å½•æ—¥å¿—
        """
        # æ£€æŸ¥è¿æ¥çŠ¶æ€
        if not self.client_wrapper.is_connected():
            return

        # è·å–æºé¢‘é“é…ç½®åˆ—è¡¨
        channels_config = self.config.get("source_channels", [])

        # ========== å¤„ç†æ¯ä¸ªé¢‘é“ ==========
        for cfg in channels_config:
            try:
                channel_name = cfg
                start_date = None

                # è§£æé¢‘é“é…ç½®ï¼ˆæ”¯æŒæ—¥æœŸè¿‡æ»¤ï¼‰
                # æ ¼å¼ï¼šchannel_name|YYYY-MM-DD
                if "|" in cfg:
                    channel_name, date_str = cfg.split("|", 1)
                    channel_name = channel_name.strip()
                    try:
                         # å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ—¶åŒºæ„ŸçŸ¥çš„ datetime å¯¹è±¡
                         start_date = datetime.strptime(date_str.strip(), "%Y-%m-%d").replace(tzinfo=timezone.utc)
                    except:
                        # æ—¥æœŸè§£æå¤±è´¥æ—¶å¿½ç•¥ï¼Œä½¿ç”¨é»˜è®¤è¡Œä¸º
                        pass
                else:
                    channel_name = cfg.strip()

                # å¤„ç†è¯¥é¢‘é“
                await self._process_channel(channel_name, start_date)
            except Exception as e:
                # è®°å½•é”™è¯¯ä½†ç»§ç»­å¤„ç†å…¶ä»–é¢‘é“
                logger.error(f"Error checking {cfg}: {e}")

            # æ³¨æ„ï¼šé€Ÿç‡é™åˆ¶ç°åœ¨åœ¨æ¯ä¸ªæ¶ˆæ¯å¤„ç†åè¿›è¡Œï¼ˆ_process_channel å†…éƒ¨ï¼‰
            # delay = self.config.get("forward_delay", 2)
            # await asyncio.sleep(delay)

    async def _process_channel(self, channel_name: str, start_date: Optional[datetime]):
        """
        å¤„ç†å•ä¸ªé¢‘é“çš„æ¶ˆæ¯æ›´æ–°

        Args:
            channel_name: é¢‘é“åç§°æˆ–ID
            start_date: å¯é€‰çš„å¼€å§‹æ—¥æœŸï¼Œç”¨äºå†·å¯åŠ¨æ—¶ä»æŒ‡å®šæ—¥æœŸè·å–æ¶ˆæ¯

        æ‰§è¡Œæµç¨‹ï¼š
        1. åˆå§‹åŒ–æˆ–è·å–é¢‘é“æœ€åå¤„ç†çš„æ¶ˆæ¯ID
        2. å¤„ç†å†·å¯åŠ¨é€»è¾‘ï¼ˆé¦–æ¬¡è¿è¡Œï¼‰
        3. è·å–æ–°æ¶ˆæ¯
        4. åº”ç”¨è¿‡æ»¤è§„åˆ™ï¼ˆå…³é”®è¯ã€æ­£åˆ™ï¼‰
        5. é€æ¡è½¬å‘å¹¶æ›´æ–°æŒä¹…åŒ–çŠ¶æ€

        å†·å¯åŠ¨é€»è¾‘ï¼š
            - æœ‰æ—¥æœŸï¼šä»æŒ‡å®šæ—¥æœŸå¼€å§‹è·å–
            - æ— æ—¥æœŸï¼šåªè·å–æœ€æ–°æ¶ˆæ¯IDï¼Œä¸å¤„ç†å†å²
        """
        # ========== åˆå§‹åŒ–é¢‘é“çŠ¶æ€ ==========
        if not self.storage.get_channel_data(channel_name).get("last_post_id"):
             self.storage.update_last_id(channel_name, 0)  # ç¡®ä¿åˆå§‹åŒ–

        last_id = self.storage.get_channel_data(channel_name)["last_post_id"]

        try:
            # ========== å†·å¯åŠ¨å¤„ç† ==========
            if last_id == 0:
                if start_date:
                    # æœ‰æ—¥æœŸé…ç½®ï¼šä»æŒ‡å®šæ—¥æœŸå¼€å§‹è·å–å†å²æ¶ˆæ¯
                    logger.info(f"Cold start for {channel_name} with date {start_date}")
                    pass  # é€»è¾‘åœ¨è¿­ä»£å‚æ•°ä¸­å¤„ç†
                else:
                    # æ— æ—¥æœŸé…ç½®ï¼šè·³è¿‡å†å²ï¼Œåªè®°å½•æœ€æ–°æ¶ˆæ¯ID
                    # è¿™æ ·å¯ä»¥é¿å…é¦–æ¬¡å¯åŠ¨æ—¶è½¬å‘å¤§é‡å†å²æ¶ˆæ¯
                     msgs = await self.client.get_messages(channel_name, limit=1)
                     if msgs:
                         self.storage.update_last_id(channel_name, msgs[0].id)
                         logger.info(f"Initialized {channel_name} at ID {msgs[0].id}")
                     return

            # ========== è·å–æ–°æ¶ˆæ¯ ==========
            new_messages = []

            # æ„å»ºæ¶ˆæ¯è¿­ä»£å‚æ•°
            params = {"entity": channel_name, "reverse": True, "limit": 20}

            if last_id > 0:
                 # å¸¸è§„è¿è¡Œï¼šè·å– ID å¤§äº last_id çš„æ–°æ¶ˆæ¯
                 params["min_id"] = last_id
            elif start_date:
                 # å†·å¯åŠ¨æœ‰æ—¥æœŸï¼šä»è¯¥æ—¥æœŸå¼€å§‹è·å–
                 params["offset_date"] = start_date
            else:
                 # å†·å¯åŠ¨æ— æ—¥æœŸï¼šè·å–å°‘é‡æœ€æ–°æ¶ˆæ¯
                 params["limit"] = 5

            # ä½¿ç”¨è¿­ä»£å™¨è·å–æ¶ˆæ¯ï¼ˆæ”¯æŒåˆ†é¡µï¼Œå†…å­˜å‹å¥½ï¼‰
            async for message in self.client.iter_messages(**params):
                if not message.id: continue
                new_messages.append(message)

            # æ²¡æœ‰æ–°æ¶ˆæ¯åˆ™è¿”å›
            if not new_messages:
                return

            # ========== è·å–è¿‡æ»¤é…ç½® ==========
            filter_keywords = self.config.get("filter_keywords", [])
            filter_regex = self.config.get("filter_regex", "")

            final_last_id = last_id

            # ========== å¤„ç†æ¯æ¡æ¶ˆæ¯ ==========
            for msg in new_messages:
                try:
                    # ----- ååƒåœ¾ / é¢‘é“è¿‡æ»¤ -----
                    # åªè½¬å‘é¢‘é“å¸–å­ï¼ˆpost=Trueï¼‰ï¼Œå¿½ç•¥ç”¨æˆ·æ¶ˆæ¯
                    is_user_msg = isinstance(msg.from_id, PeerUser) if msg.from_id else False

                    if not msg.post and is_user_msg:
                        continue

                    text_content = msg.text or ""

                    # ----- å…³é”®è¯è¿‡æ»¤ -----
                    should_skip = False
                    if filter_keywords:
                        for kw in filter_keywords:
                            if kw in text_content:
                                logger.info(f"Filtered {msg.id}: Keyword {kw}")
                                should_skip = True
                                break

                    # ----- æ­£åˆ™è¡¨è¾¾å¼è¿‡æ»¤ -----
                    if not should_skip and filter_regex:
                        if re.search(filter_regex, text_content, re.IGNORECASE | re.DOTALL):
                            logger.info(f"Filtered {msg.id}: Regex")
                            should_skip = True

                    # ----- è½¬å‘æ¶ˆæ¯ -----
                    if not should_skip:
                         await self._forward_message(channel_name, msg)

                    # ----- ç«‹å³æ›´æ–°æŒä¹…åŒ–çŠ¶æ€ -----
                    # æ¯å¤„ç†ä¸€æ¡æ¶ˆæ¯å°±ä¿å­˜ï¼Œé¿å…ä¸­é€”å¤±è´¥å¯¼è‡´é‡å¤
                    final_last_id = max(final_last_id, msg.id)
                    self.storage.update_last_id(channel_name, final_last_id)

                    # ----- é€Ÿç‡é™åˆ¶ / é˜²æ‰“æ‰° -----
                    # æ¯æ¡æ¶ˆæ¯å¤„ç†å®Œåå»¶è¿Ÿï¼Œé¿å…è¢«å¹³å°é™åˆ¶
                    delay = self.config.get("forward_delay", 0)
                    if delay > 0:
                        logger.info(f"Rate limit: sleeping {delay}s...")
                        await asyncio.sleep(delay)

                except Exception as e:
                    # å•æ¡æ¶ˆæ¯å¤„ç†å¤±è´¥ä¸å½±å“å…¶ä»–æ¶ˆæ¯
                    logger.error(f"Failed to process msg {msg.id}: {e}")

        except Exception as e:
            # é¢‘é“è®¿é—®é”™è¯¯ï¼ˆå¦‚æ— æƒé™ã€é¢‘é“ä¸å­˜åœ¨ç­‰ï¼‰
            logger.error(f"Access error for {channel_name}: {e}")

    async def _forward_message(self, src_channel: str, msg: Message):
        """
        ç¼–æ’æ¶ˆæ¯è½¬å‘åˆ°æ‰€æœ‰ç›®æ ‡å¹³å°

        Args:
            src_channel: æºé¢‘é“åç§°
            msg: Telegram æ¶ˆæ¯å¯¹è±¡

        Note:
            æ­¤æ–¹æ³•æ˜¯è½¬å‘é€»è¾‘çš„å…¥å£ç‚¹ï¼ŒæŒ‰é¡ºåºè°ƒç”¨å„å¹³å°è½¬å‘æ–¹æ³•
        """
        await self._forward_to_telegram(src_channel, msg)
        await self._forward_to_qq(src_channel, msg)

    async def _forward_to_telegram(self, src_channel: str, msg: Message):
        """
        è½¬å‘æ¶ˆæ¯åˆ° Telegram ç›®æ ‡é¢‘é“

        Args:
            src_channel: æºé¢‘é“åç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰
            msg: è¦è½¬å‘çš„æ¶ˆæ¯å¯¹è±¡

        è½¬å‘æ–¹å¼ï¼š
            ä½¿ç”¨ Telethon çš„ forward_messages æ–¹æ³•ï¼Œä¿ç•™åŸå§‹æ¶ˆæ¯çš„æ‰€æœ‰å±æ€§
            ï¼ˆåª’ä½“ã€è½¬å‘å¼•ç”¨ã€å›å¤ç­‰ï¼‰

        ç›®æ ‡æ ¼å¼æ”¯æŒï¼š
            - @channelname (é¢‘é“å)
            - -1001234567890 (é¢‘é“æ•°å­—ID)
        """
        tg_target = self.config.get("target_channel")
        bot_token = self.config.get("bot_token")

        # åªæœ‰é…ç½®äº†ç›®æ ‡é¢‘é“å’Œ bot_token æ—¶æ‰è½¬å‘
        if tg_target and bot_token:
            try:
                 # ========== è§£æç›®æ ‡é¢‘é“ ==========
                 # å…ˆè§£æç›®æ ‡å®ä½“ï¼Œé¿å… "Cannot find entity" é”™è¯¯
                 target = tg_target
                 if isinstance(target, str):
                    # å¤„ç†æ•°å­—IDæ ¼å¼çš„é¢‘é“
                    if target.startswith("-") or target.isdigit():
                        try:
                            target = int(target)
                        except:
                            pass

                 # è·å–ç›®æ ‡å®ä½“å¹¶è½¬å‘æ¶ˆæ¯
                 entity = await self.client.get_entity(target)
                 await self.client.forward_messages(entity, msg)
                 logger.info(f"Forwarded {msg.id} to TG")
            except Exception as e:
                 # è½¬å‘å¤±è´¥è®°å½•é”™è¯¯ä½†ä¸ä¸­æ–­ç¨‹åº
                 logger.error(f"TG Forward Error: {e}")

    async def _forward_to_qq(self, src_channel: str, msg: Message):
        """
        è½¬å‘æ¶ˆæ¯åˆ° QQ ç¾¤

        Args:
            src_channel: æºé¢‘é“åç§°
            msg: Telegram æ¶ˆæ¯å¯¹è±¡

        æ‰§è¡Œæµç¨‹ï¼š
        1. ä¸‹è½½åª’ä½“æ–‡ä»¶ï¼ˆå¸¦å¤§å°æ£€æŸ¥ï¼‰
        2. æ¸…ç†æ–‡æœ¬å†…å®¹
        3. å¤„ç†æ–‡ä»¶ï¼ˆå°æ–‡ä»¶ base64ï¼Œå¤§æ–‡ä»¶ä¸Šä¼ æ‰˜ç®¡ï¼‰
        4. æ„å»º NapCat æ¶ˆæ¯æ ¼å¼
        5. å‘é€åˆ° QQ ç¾¤
        6. æ¸…ç†ä¸´æ—¶æ–‡ä»¶

        æ¶ˆæ¯æ ¼å¼ï¼š
            [
                {"type": "text", "data": {"text": "æ¶ˆæ¯å†…å®¹"}},
                {"type": "image", "data": {"file": "base64://..."}},
                {"type": "text", "data": {"text": "[åª’ä½“é“¾æ¥]"}}
            ]

        NapCat API:
            ä½¿ç”¨ OneBot 11 æ ‡å‡†åè®®çš„ send_group_msg æ¥å£
        """
        qq_groups = self.config.get("target_qq_group")
        napcat_url = self.config.get("napcat_api_url")

        # å¿…é¡»åŒæ—¶é…ç½® QQ ç¾¤å’Œ API åœ°å€
        if not (qq_groups and napcat_url):
            return
            
        # å…¼å®¹æ—§é…ç½®ï¼ˆå•ä¸ª intï¼‰
        if isinstance(qq_groups, int):
            qq_groups = [qq_groups]
        elif not isinstance(qq_groups, list):
            return

        local_files = []
        try:
             # ========== ä¸‹è½½åª’ä½“æ–‡ä»¶ï¼ˆå¸¦å®‰å…¨æ£€æŸ¥ï¼‰ ==========
            local_files = await self._download_media_safe(msg)

            # ========== æ¸…ç†å’Œå‡†å¤‡æ–‡æœ¬ ==========
            header = f"From #{src_channel}:\n"
            cleaned_text = clean_telegram_text(msg.text or "")
            final_text = header + cleaned_text

            # ç©ºå†…å®¹æ£€æŸ¥
            if not final_text and not local_files:
                logger.info("Skipped forwarding: Empty content after cleaning")
                return

            # ========== æ„å»ºæ¶ˆæ¯è½½è· ==========
            message = [{"type": "text", "data": {"text": final_text}}]

            # ========== å¤„ç†æ–‡ä»¶ï¼ˆä¸Šä¼ æˆ– Base64ï¼‰ ==========
            for fpath in local_files:
                file_nodes = await self._process_one_file(fpath)
                if file_nodes:
                    message.extend(file_nodes)

            # ========== å‘é€åˆ° QQ ==========
            # ========== å‘é€åˆ° QQ ==========
            url = self.config.get("napcat_api_url", "http://127.0.0.1:3000/send_group_msg")
            async with httpx.AsyncClient() as http:
                 for gid in qq_groups:
                     if not gid: continue
                     try:
                        # æ£€æŸ¥æ˜¯å¦æœ‰ record èŠ‚ç‚¹
                        has_record = any(node.get("type") == "record" for node in message)
                        
                        if has_record:
                            # å¦‚æœæœ‰è¯­éŸ³ï¼Œå…ˆå‘é€æ–‡æœ¬éƒ¨åˆ†ï¼Œå†å•ç‹¬å‘é€è¯­éŸ³
                            # 1. æå–æ–‡æœ¬èŠ‚ç‚¹
                            text_nodes = [node for node in message if node.get("type") == "text"]
                            if text_nodes:
                                await http.post(url, json={"group_id": gid, "message": text_nodes}, timeout=30)
                                await asyncio.sleep(1) # ç¨å¾®å»¶è¿Ÿï¼Œä¿è¯é¡ºåº

                            # 2. æå–å¹¶å‘é€è¯­éŸ³èŠ‚ç‚¹
                            record_nodes = [node for node in message if node.get("type") == "record"]
                            for rec_node in record_nodes:
                                await http.post(url, json={"group_id": gid, "message": [rec_node]}, timeout=30)
                            
                            logger.info(f"Forwarded {msg.id} to QQ group {gid} (Split Text/Record)")
                        else:
                            # æ™®é€šæ¶ˆæ¯ç›´æ¥å‘é€
                            await http.post(url, json={"group_id": gid, "message": message}, timeout=30)
                            logger.info(f"Forwarded {msg.id} to QQ group {gid}")

                     except Exception as e:
                        logger.error(f"Failed to send to QQ group {gid}: {e}")

        except Exception as e:
            logger.error(f"QQ Forward Error: {e}")
        finally:
            # ========== æ¸…ç†ä¸´æ—¶æ–‡ä»¶ ==========
            self._cleanup_files(local_files)

    async def _download_media_safe(self, msg: Message) -> list:
        """
        ä¸‹è½½åª’ä½“æ–‡ä»¶ï¼ˆå¸¦å¤§å°æ£€æŸ¥ï¼‰

        Args:
            msg: Telegram æ¶ˆæ¯å¯¹è±¡

        Returns:
            list: ä¸‹è½½çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨

        å®‰å…¨æªæ–½ï¼š
            - æ–‡ä»¶å¤§å°é™åˆ¶ï¼š50MB
            - åªä¸‹è½½å›¾ç‰‡ï¼ˆphotoï¼‰ï¼Œä¸ä¸‹è½½è§†é¢‘/æ–‡æ¡£
            - ä¸‹è½½è¿›åº¦å›è°ƒï¼ˆæ¯20%è¾“å‡ºä¸€æ¬¡ï¼‰

        Note:
            ä¸ºäº†é¿å…ä¸‹è½½å¤§æ–‡ä»¶å¯¼è‡´ç£ç›˜ç©ºé—´æˆ–æ€§èƒ½é—®é¢˜ï¼Œ
            å½“å‰åªæ”¯æŒå›¾ç‰‡ç±»å‹ã€‚å…¶ä»–ç±»å‹ä¼šè·³è¿‡ã€‚
        """
        local_files = []

        # æ£€æŸ¥æ¶ˆæ¯æ˜¯å¦åŒ…å«åª’ä½“
        if not msg.media:
            return local_files

        # ========== æ–‡ä»¶å¤§å°æ£€æŸ¥ ==========
        if hasattr(msg.media, 'document') and hasattr(msg.media.document, 'size'):
            if msg.media.document.size > MAX_FILE_SIZE:
                logger.warning(f"File too large ({msg.media.document.size} bytes), skipping download.")
                return local_files

        # ========== åˆ¤æ–­æ˜¯å¦åº”è¯¥ä¸‹è½½ ==========
        # æ”¯æŒå›¾ç‰‡å’ŒéŸ³é¢‘
        is_photo = hasattr(msg, 'photo') and msg.photo
        is_audio = False
        
        # æ£€æŸ¥éŸ³é¢‘/è¯­éŸ³
        if msg.file and msg.file.mime_type:
            mime = msg.file.mime_type
            if mime.startswith('audio/') or mime == 'application/ogg':
                is_audio = True

        should_download = is_photo or is_audio

        if should_download:
             # å®šä¹‰è¿›åº¦å›è°ƒå‡½æ•°
             def progress_callback(current, total):
                if total > 0:
                    pct = (current / total) * 100
                    # æ¯ 20% è¾“å‡ºä¸€æ¬¡è¿›åº¦ï¼Œé¿å…æ—¥å¿—åˆ·å±
                    if int(pct) % 20 == 0 and int(pct) > 0:
                        logger.info(f"Downloading {msg.id}: {pct:.1f}%")

             # æ‰§è¡Œä¸‹è½½
             try:
                path = await self.client.download_media(
                    msg,
                    file=self.plugin_data_dir,
                    progress_callback=progress_callback
                )
                if path:
                    local_files.append(path)
             except Exception as e:
                logger.error(f"Download failed for msg {msg.id}: {e}")

        return local_files

    async def _process_one_file(self, fpath: str) -> list:
        """
        å°†æœ¬åœ°æ–‡ä»¶è½¬æ¢ä¸º NapCat æ¶ˆæ¯èŠ‚ç‚¹åˆ—è¡¨

        Args:
            fpath: æ–‡ä»¶è·¯å¾„

        Returns:
            list: NapCat æ¶ˆæ¯èŠ‚ç‚¹åˆ—è¡¨ï¼Œæ¯é¡¹å¦‚ {"type": "image", "data": {...}}

        å¤„ç†ç­–ç•¥ï¼š
            1. å›¾ç‰‡æ–‡ä»¶ï¼ˆ<5MBï¼‰ï¼šä½¿ç”¨ Base64 ç¼–ç ç›´æ¥åµŒå…¥
            2. éŸ³é¢‘æ–‡ä»¶ï¼šä¸Šä¼ åç”Ÿæˆ [è¯­éŸ³æ¶ˆæ¯ + é“¾æ¥]
            3. å…¶ä»–æ–‡ä»¶ï¼šä¸Šä¼ åˆ°æ–‡ä»¶æ‰˜ç®¡æœåŠ¡ï¼ˆå¦‚æœé…ç½®ï¼‰
            4. æ— æ‰˜ç®¡ï¼šè¿”å›æ–‡ä»¶åå ä½ç¬¦
        """
        ext = os.path.splitext(fpath)[1].lower()
        hosting_url = self.config.get("file_hosting_url")

        # ========== 1. å›¾ç‰‡ -> Base64ï¼ˆå°æ–‡ä»¶å®‰å…¨ï¼‰ ==========
        if ext in [".jpg", ".jpeg", ".png", ".webp", ".gif", ".bmp"]:
             # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼ŒBase64 å¯¹å¤§æ–‡ä»¶ä¸å‹å¥½
            if os.path.getsize(fpath) < 5 * 1024 * 1024:
                import base64
                with open(fpath, "rb") as image_file:
                    encoded_string = base64.b64encode(image_file.read()).decode('utf-8')
                # NapCat å›¾ç‰‡æ¶ˆæ¯æ ¼å¼
                return [{"type": "image", "data": {"file": f"base64://{encoded_string}"}}]
            else:
                logger.info("Image too large for base64, trying upload...")

        # ========== 2. ä¸Šä¼ åˆ°æ–‡ä»¶æ‰˜ç®¡æœåŠ¡ ==========
        if hosting_url:
            try:
                async with httpx.AsyncClient() as uploader:
                    # Check file size for chunked upload (> 5MB)
                    if os.path.getsize(fpath) > 5 * 1024 * 1024:
                        logger.info(f"File > 5MB, using Chunked Upload for {fpath}...")
                        link = await self._upload_chunked(uploader, hosting_url, fpath)
                    else:
                        # æ™®é€šä¸Šä¼ 
                        # æ„å»º multipart/form-data ä¸Šä¼ 
                        with open(fpath, "rb") as f:
                            files = {'file': (os.path.basename(fpath), f, 'application/octet-stream')}
                            
                            # æ·»åŠ ç›®å½•å‚æ•°ï¼Œå°†å…¶ä½œä¸º Query Param ä¼ é€’
                            # httpx ä¼šè‡ªåŠ¨åˆå¹¶ url ä¸­çš„ query å’Œ params å‚æ•°
                            params = {'uploadFolder': 'Telegram/Media'}
                            
                            # ä¸Šä¼ è¶…æ—¶è®¾ç½®ä¸º 120 ç§’ï¼ˆé€‚åº”æ…¢é€Ÿç½‘ç»œï¼‰
                            resp = await uploader.post(hosting_url, files=files, params=params, timeout=120)
    
                            if resp.status_code == 200:
                                # æå–ä¸Šä¼ åçš„ URL
                                link = self._extract_upload_url(resp.json(), hosting_url)
                            else:
                                logger.error(f"Upload failed: {resp.status_code} {resp.text}")
                                link = None

                    if link:
                        # å¦‚æœæ˜¯éŸ³é¢‘ï¼Œå°è¯•å‘é€è¯­éŸ³é¢„è§ˆ + é“¾æ¥
                        if ext in [".mp3", ".ogg", ".wav", ".m4a", ".flac", ".amr"]:
                                logger.info(f"Audio Link Generated: {link}")
                                return [
                                    {"type": "text", "data": {"text": f"\n[Audio: {os.path.basename(fpath)}]\nğŸ”— Link: {link}\n"}},
                                    {"type": "record", "data": {"file": link}}
                                ]
                        
                        # æ™®é€šæ–‡ä»¶/å¤§å›¾ç‰‡
                        return [{"type": "text", "data": {"text": f"\n[Media Link: {link}]"}}]
                    else:
                         # ä¸Šä¼ å¤±è´¥
                         status = resp.status_code if 'resp' in locals() else "Unknown"
                         return [{"type": "text", "data": {"text": f"\n[Upload Failed: HTTP {status}]"}}]
            except Exception as e:
                 logger.error(f"Upload Error: {e}")
                 return [{"type": "text", "data": {"text": f"\n[Media File: {os.path.basename(fpath)}] (Upload Failed)"}}]


        # ========== 3. å›é€€æ–¹æ¡ˆ ==========
        # æ— æ‰˜ç®¡æœåŠ¡æ—¶ï¼Œè¿”å›æ–‡ä»¶åå ä½ç¬¦
        fname = os.path.basename(fpath)
        return [{"type": "text", "data": {"text": f"\n[Media File: {fname}] (Too large/No hosting)"}}]

    def _extract_upload_url(self, res_json: dict, base_url: str) -> str:
        """
        ä»å„ç§æ‰˜ç®¡æœåŠ¡å“åº”æ ¼å¼ä¸­æå– URL

        Args:
            res_json: ä¸Šä¼ æ¥å£è¿”å›çš„ JSON å“åº”
            base_url: æ‰˜ç®¡æœåŠ¡çš„ URLï¼Œç”¨äºå¤„ç†ç›¸å¯¹è·¯å¾„

        Returns:
            str: æå–åˆ°çš„æ–‡ä»¶ URL

        æ”¯æŒçš„å“åº”æ ¼å¼ï¼š
            1. Telegraph.ph: [{"src": "/file/..."}]
            2. ç®€å•æ ¼å¼ï¼š{"url": "https://..."}
            3. æ ‡å‡† APIï¼š{"code": 200, "data": {"url": "..."}}

        çµæ´»æ€§ï¼š
            - è‡ªåŠ¨å¤„ç†ç›¸å¯¹è·¯å¾„å’Œç»å¯¹è·¯å¾„
            - å®¹é”™å¤„ç†ï¼šæ ¼å¼ä¸åŒ¹é…æ—¶å°è¯•ä¸‹ä¸€ä¸ªæå–å™¨
        """
        # ========== ç¡®å®šæ ¹ URLï¼ˆç”¨äºç›¸å¯¹è·¯å¾„ï¼‰ ==========
        from urllib.parse import urlparse
        parsed = urlparse(base_url)
        root_url = f"{parsed.scheme}://{parsed.netloc}"

        # ========== å®šä¹‰ URL æå–å™¨åˆ—è¡¨ ==========
        extractors = [
            # Telegraph.ph æ ¼å¼ï¼šåˆ—è¡¨ + ç›¸å¯¹è·¯å¾„
            lambda r: root_url + r[0]["src"] if isinstance(r, list) and r and "src" in r[0] else None,

            # ç®€å•æ ¼å¼ï¼šç›´æ¥è¿”å› url å­—æ®µ
            lambda r: r.get("url"),

            # æ ‡å‡† API æ ¼å¼ï¼šåµŒå¥—åœ¨ data ä¸­
            lambda r: r.get("data", {}).get("url") if isinstance(r.get("data"), dict) else None,

            # 4. Chunked Upload polling result (Dict with result List)
            lambda r: root_url + r["result"][0]["src"] if isinstance(r, dict) and "result" in r and isinstance(r["result"], list) and r["result"] and "src" in r["result"][0] else None
        ]

        # ========== ä¾æ¬¡å°è¯•æå–å™¨ ==========
        for extractor in extractors:
            try:
                link = extractor(res_json)
                if link:
                    # å¤„ç†ç›¸å¯¹è·¯å¾„
                    if link.startswith("/"):
                        return root_url + link
                    # å¤„ç†ç»å¯¹è·¯å¾„
                    if link.startswith("http"):
                        return link
            except:
                # å½“å‰æå–å™¨å¤±è´¥ï¼Œç»§ç»­å°è¯•ä¸‹ä¸€ä¸ª
                continue

        # ========== æ‰€æœ‰æ ¼å¼éƒ½ä¸åŒ¹é… ==========
        logger.warning(f"Unknown upload response format: {res_json}")
        return "[Unknown Link]"

    async def _upload_chunked(self, uploader: httpx.AsyncClient, hosting_url: str, fpath: str) -> Optional[str]:
        """
        Chunked upload for large files.
        Init -> Chunks -> Merge -> Poll
        """
        import math
        chunk_size = 5 * 1024 * 1024  # 5MB
        file_size = os.path.getsize(fpath)
        total_chunks = math.ceil(file_size / chunk_size)
        original_filename = os.path.basename(fpath)
        # Simple mimetype guess
        ext = os.path.splitext(fpath)[1].lower()
        original_filetype = "application/octet-stream"
        if ext == ".flac": original_filetype = "audio/flac"
        elif ext == ".mp3": original_filetype = "audio/mpeg"
        
        try:
            # 1. Init (Hybrid: Query+Body)
            params = {
                'uploadFolder': 'Telegram/Media',
                'initChunked': 'true'
            }
            data = {
                'totalChunks': str(total_chunks),
                'originalFileName': original_filename,
                'originalFileType': original_filetype
            }
            dummy_files = {'_force_multipart': ('', b'')}
            
            resp = await uploader.post(hosting_url, params=params, data=data, files=dummy_files, timeout=30)
            if resp.status_code != 200:
                logger.error(f"Chunked Init failed: {resp.status_code} {resp.text}")
                return None
            
            resp_data = resp.json()
            upload_id = None
            if isinstance(resp_data, dict):
                 upload_id = resp_data.get("data") or resp_data.get("uploadId")
            if not upload_id:
                 logger.error(f"Chunked Init no uploadId: {resp.text}")
                 return None

            # 2. Upload Chunks
            with open(fpath, "rb") as f:
                for i in range(total_chunks):
                    chunk_data = f.read(chunk_size)
                    q_params = {'uploadFolder': 'Telegram/Media', 'chunked': 'true'}
                    b_data = {
                        'uploadId': upload_id,
                        'chunkIndex': str(i),
                        'totalChunks': str(total_chunks),
                        'originalFileName': original_filename,
                        'originalFileType': original_filetype
                    }
                    files = {'file': (original_filename, chunk_data, original_filetype)}
                    
                    c_resp = await uploader.post(hosting_url, params=q_params, data=b_data, files=files, timeout=120)
                    if c_resp.status_code != 200:
                        logger.error(f"Chunk {i} failed: {c_resp.status_code} {c_resp.text}")
                        return None
            
            # 3. Merge
            m_params = {'uploadFolder': 'Telegram/Media', 'chunked': 'true', 'merge': 'true'}
            m_data = {
                'uploadId': upload_id,
                'totalChunks': str(total_chunks),
                'originalFileName': original_filename,
                'originalFileType': original_filetype
            }
            dummy_files_m = {'_force_multipart': ('', b'')}
            m_resp = await uploader.post(hosting_url, params=m_params, data=m_data, files=dummy_files_m, timeout=60)
            
            final_resp = None
            if m_resp.status_code == 200:
                final_resp = m_resp.json()
            elif m_resp.status_code == 202:
                # Async polling
                check_url = m_resp.json().get("statusCheckUrl")
                if not check_url:
                     check_url = f"/upload?uploadId={upload_id}&statusCheck=true&chunked=true&merge=true"
                
                # Handle relative URL
                if check_url.startswith("/"):
                     from urllib.parse import urlparse
                     parsed = urlparse(hosting_url)
                     root = f"{parsed.scheme}://{parsed.netloc}"
                     check_url = root + check_url
                
                # Append AuthCode if missing but present in hosting_url
                if "authCode=" not in check_url and "authCode=" in hosting_url:
                      auth_code = hosting_url.split("authCode=")[1].split("&")[0]
                      if "?" in check_url: check_url += f"&authCode={auth_code}"
                      else: check_url += f"?authCode={auth_code}"
                
                # Poll
                for retry in range(20): # Retry 20 times * 2s = 40s
                    await asyncio.sleep(2)
                    try:
                        poll_resp = await uploader.get(check_url, timeout=30)
                        if poll_resp.status_code == 200:
                            json_data = poll_resp.json()
                            if json_data.get("status") == "success" or json_data.get("url"):
                                final_resp = json_data
                                break
                        elif poll_resp.status_code != 202:
                            logger.error(f"Poll failed: {poll_resp.status_code}")
                            return None
                    except Exception as e:
                        logger.error(f"Poll error: {e}")
            
            if final_resp:
                return self._extract_upload_url(final_resp, hosting_url)
            else:
                logger.error("Merge failed or timed out")
                return None
        except Exception as e:
            logger.error(f"Chunked Upload Exception: {e}")
            return None

    def _cleanup_files(self, files: list):
        """
        æ¸…ç†ä¸´æ—¶ä¸‹è½½çš„æ–‡ä»¶

        Args:
            files: æ–‡ä»¶è·¯å¾„åˆ—è¡¨

        è¡Œä¸ºï¼š
            - åˆ é™¤æ¯ä¸ªå­˜åœ¨çš„ä¸´æ—¶æ–‡ä»¶
            - é™é»˜å¤„ç†åˆ é™¤å¤±è´¥ï¼ˆæ–‡ä»¶å¯èƒ½å·²è¢«å…¶ä»–è¿›ç¨‹å ç”¨ï¼‰
        """
        for f in files:
            if os.path.exists(f):
                try:
                    os.remove(f)
                except:
                    # åˆ é™¤å¤±è´¥æ—¶é™é»˜å¿½ç•¥ï¼Œä¸ä¸­æ–­æµç¨‹
                    pass
